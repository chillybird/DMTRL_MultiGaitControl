# Prepare parameters:
#    --algorithm_name <algorithm_name>
#        specifiy the algorithm, including `["rmappo", "mappo", "rmappg", "mappg", "trpo"]`
#    --experiment_name <str>
#        an identifier to distinguish different experiment.
#    --seed <int>
#        set seed for numpy and torch
#    --cuda
#        by default true, will use GPU to train; or else will use CPU;
#    --cuda_deterministic
#        by default, make sure random seed effective. if set, bypass such function.
#    --n_training_threads <int>
#        number of training threads working in parallel. by default 1
#    --n_rollout_threads <int>
#        number of parallel envs for training rollout. by default 32
#    --n_eval_rollout_threads <int>
#        number of parallel envs for evaluating rollout. by default 1
#    --n_render_rollout_threads <int>
#        number of parallel envs for rendering, could only be set as 1 for some environments.
#    --num_env_steps <int>
#        number of env steps to train (default: 10e6)
#    --user_name <str>
#        [for wandb usage], to specify user's name for simply collecting training data.
#    --use_wandb
#        [for wandb usage], by default true, will log date to wandb server. or else will use tensorboard to log data.

algorithm_name: rmappo
experiment_name: check
seed: 1
cuda: true
cuda_deterministic: true
n_training_threads: 1
n_rollout_threads: 5
# n_eval_rollout_threads: 1
n_render_rollout_threads: 1
num_env_steps: 10000000

user_name: marl,
use_wandb: true,
#Env parameters:
#    --env_name <str>
#        specify the name of environment
#    --use_obs_instead_of_state
#        [only for some env] by default false, will use global state; or else will use concatenated local obs.
env_name: MyEnv
use_obs_instead_of_state: false

#Replay Buffer parameters:
#    --episode_length <int>
#        the max length of episode in the buffer.
episode_length: 200

#Network parameters:
#    --share_policy
#        by default true, all agents will share the same network; set to make training agents use different policies.
#    --use_centralized_V
#        by default true, use centralized training mode; or else will decentralized training mode.
#    --stacked_frames <int>
#        Number of input frames which should be stack together.
#    --hidden_size <int>
#        Dimension of hidden layers for actor/critic networks
#    --layer_N <int>
#        Number of layers for actor/critic networks
#    --use_ReLU
#        by default true, will use ReLU. or else will use Tanh.
#    --use_popart
#        by default true, use PopArt to normalize rewards.
#    --use_valuenorm
#        by default true, use running mean and std to normalize rewards.
#    --use_feature_normalization
#        by default true, apply layernorm to normalize inputs.
#    --use_orthogonal
#        by default true, use Orthogonal initialization for weights and 0 initialization for biases. or else, will use xavier uniform inilialization.
#    --gain
#        by default 0.01, use the gain # of last action layer
#    --use_naive_recurrent_policy
#        by default false, use the whole trajectory to calculate hidden states.
#    --use_recurrent_policy
#        by default, use Recurrent Policy. If set, do not use.
#    --recurrent_N <int>
#        The number of recurrent layers ( default 1).
#    --data_chunk_length <int>
#        Time length of chunks used to train a recurrent_policy, default 10.
share_policy: true
# represent every agent's observation is concatenated all agents' observations
use_centralized_V: true
stacked_frames: 1
use_stacked_frames: false
hidden_size: 64
layer_N: 1
use_ReLU: true
use_popart: true
use_valuenorm: true

use_feature_normalization: true
use_orthogonal: true
gain: 0.01

use_naive_recurrent_policy: false
use_recurrent_policy: true
recurrent_N: 1
data_chunk_length: 10

#Optimizer parameters:
#    --lr <float>
#        learning rate parameter,  (default: 5e-4, fixed).
#    --critic_lr <float>
#        learning rate of critic  (default: 5e-4, fixed)
#    --opti_eps <float>
#        RMSprop optimizer epsilon (default: 1e-5)
#    --weight_decay <float>
#        coefficience of weight decay (default: 0)
lr: 0.0005
critic_lr: 0.0005
opti_eps: 0.00001
weight_decay: 0
std_x_coef: 1.0
std_y_coef: 0.5


#PPO parameters:
#    --ppo_epoch <int>
#        number of ppo epochs (default: 15)
#    --use_clipped_value_loss
#        by default, clip loss value. If set, do not clip loss value.
#    --clip_param <float>
#        ppo clip parameter (default: 0.2)
#    --num_mini_batch <int>
#        number of batches for ppo (default: 1)
#    --entropy_coef <float>
#        entropy term coefficient (default: 0.01)
#    --use_max_grad_norm
#        by default, use max norm of gradients. If set, do not use.
#    --max_grad_norm <float>
#        max norm of gradients (default: 0.5)
#    --use_gae
#        by default, use generalized advantage estimation. If set, do not use gae.
#    --gamma <float>
#        discount factor for rewards (default: 0.99)
#    --gae_lambda <float>
#        gae lambda parameter (default: 0.95)
#    --use_proper_time_limits
#        by default, the return value does consider limits of time. If set, compute returns with considering time limits factor.
#    --use_huber_loss
#        by default, use huber loss. If set, do not use huber loss.
#    --use_value_active_masks
#        by default true, whether to mask useless data in value loss.
#    --huber_delta <float>
#        coefficient of huber loss.
ppo_epoch: 15
use_clipped_value_loss: true
clip_param: 0.2
num_mini_batch: 1
entropy_coef: 0.01
value_loss_coef: 1
use_max_grad_norm: true
max_grad_norm: 10.0
use_gae: true
gamma: 0.99
gae_lambda: 0.95
use_proper_time_limits: false
use_huber_loss: true
use_value_active_masks: true
use_policy_active_masks: true
huber_delta: 10.0

#PPG parameters:
#  --aux_epoch <int>
#      number of auxiliary epochs. (default: 4)
#  --clone_coef <float>
#      clone term coefficient (default: 0.01)
aux_epoch: 4
clone_coef: 0.01

#Run parameters:
#  --use_linear_lr_decay
#      by default, do not apply linear decay to learning rate. If set, use a linear schedule on the learning rate
use_linear_lr_decay: false

#Save & Log parameters:
#  --save_interval <int>
#      time duration between contiunous twice models saving.
#  --log_interval <int>
#      time duration between contiunous twice log printing.
save_interval: 1
log_interval: 5

#Eval parameters:
#  --use_eval
#      by default, do not start evaluation. If set`, start evaluation alongside with training.
#  --eval_interval <int>
#      time duration between contiunous twice evaluation progress.
#  --eval_episodes <int>
#      number of episodes of a single evaluation.
use_eval: false
eval_interval: 25
eval_episodes: 32

#Render parameters:
#  --save_gifs
#      by default, do not save render video. If set, save video.
#  --use_render
#      by default, do not render the env during training. If set, start render. Note: something, the environment has internal render process which is not controlled by this hyperparam.
#  --render_episodes <int>
#      the number of episodes to render a given env
#  --ifi <float>
#      the play interval of each rendered image in saved video.
save_gifs: false
use_render: false
render_episodes: 5
ifi: 0.1

#Pretrained parameters:
#  --model_dir <str>
#      by default None. set the path to pretrained model.
model_dir: null

# self-defined args
random_start: false
use_linear_decay_action_std: true
